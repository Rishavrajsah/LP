{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4efdaf0-3f38-4497-ba6f-e6b2c6fb3004",
   "metadata": {},
   "source": [
    "Assignment 5: Implementing the Continuous Bag of Words (CBOW) Model \n",
    "Implement the Continuous Bag of Words (CBOW) model.\n",
    "\n",
    "a. Data preparation. \n",
    "b. Generate training data. \n",
    "c. Train the model. \n",
    "d. Output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dc4c1e-2f92-4d08-95d6-1469f8c333bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07987c0b-3b02-4793-b641-791e74cbbf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love deep learning\",\n",
    "    \"Deep learning loves me\",\n",
    "    \"I enjoy machine learning\",\n",
    "    \"Machine learning is amazing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a1b32f-9c0c-441b-9a7c-7ce8ad8f3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2adbfe8-c494-497a-8fec-4620e6b88df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert corpus to sequence of word IDs\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d3d461-4f07-4d38-b7a3-f4e355db75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Generate Training Data \n",
    "data = []\n",
    "for seq in sequences:\n",
    "    for i, word in enumerate(seq):\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, len(seq))\n",
    "        context = [seq[j] for j in range(start, end) if j != i]\n",
    "        target = word\n",
    "        data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184fb7b9-ce58-4fbd-88ab-3fef9ae3e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input and output\n",
    "X, y = [], []\n",
    "for context, target in data:\n",
    "    context_vec = np.zeros(vocab_size)\n",
    "    for w in context:\n",
    "        context_vec[w] += 1\n",
    "    X.append(context_vec)\n",
    "    y.append(target)\n",
    "X, y = np.array(X), np.array(y)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faee6d3d-2bab-45e0-81d9-9c6a3fd4697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e055cb5e50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c. Train the Model \n",
    "embedding_dim = 10\n",
    "inp = Input(shape=(vocab_size,))\n",
    "hidden = Dense(embedding_dim, activation='linear')(inp)\n",
    "output = Dense(vocab_size, activation='softmax')(hidden)\n",
    "\n",
    "model = Model(inputs=inp, outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training CBOW model...\")\n",
    "model.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d42db295-d5d5-4d91-9b8e-c58e8be5c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embeddings (sample):\n",
      "\n",
      "learning   → [ 0.00464135 -0.05570217 -0.16432285  0.24950802  0.32383275]\n",
      "i          → [-0.39216858  0.23683631  0.382338    0.04389751 -0.01643245]\n",
      "deep       → [-0.03806103 -0.14604086  0.25951385  0.07641445  0.23680714]\n",
      "machine    → [ 0.29325497 -0.04739945 -0.18001223 -0.45772907 -0.2553591 ]\n",
      "love       → [ 0.2621176   0.02216213 -0.37858602  0.28329006  0.4511873 ]\n",
      "loves      → [-0.50317806  0.12171028 -0.32249603 -0.11636546  0.06174104]\n",
      "me         → [ 0.45435497 -0.3104306  -0.14612693 -0.61224675 -0.1365502 ]\n",
      "enjoy      → [-0.50823635 -0.20408818 -0.22386348  0.07269451 -0.3263174 ]\n",
      "is         → [-0.62297046 -0.6165733   0.614272    0.30192378 -0.46494496]\n",
      "amazing    → [ 0.11767042 -0.43316394 -0.30302748  0.36006618  0.12427479]\n"
     ]
    }
   ],
   "source": [
    "# d. Output \n",
    "weights = model.get_weights()[0]\n",
    "print(\"\\nWord Embeddings (sample):\\n\")\n",
    "for word, idx in word2id.items():\n",
    "    print(f\"{word:10s} → {weights[idx][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef73153-6229-4808-a9ea-49e0c95702a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar words to 'learning': [('deep', np.float32(0.52126825)), ('i', np.float32(0.26465905)), ('amazing', np.float32(0.071893506))]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def similar(word):\n",
    "    if word not in word2id: return []\n",
    "    vec = weights[word2id[word]]\n",
    "    sims = {w: np.dot(vec, weights[word2id[w]]) / (norm(vec)*norm(weights[word2id[w]]))\n",
    "            for w in word2id if w != word}\n",
    "    return sorted(sims.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "print(\"\\nSimilar words to 'learning':\", similar(\"learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb77171-4d81-407a-8762-8d13dbd06a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
